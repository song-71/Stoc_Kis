"""
ÏÑ§Ï†ï ÌååÏùº Í¥ÄÎ¶¨ Ïú†Ìã∏Î¶¨Ìã∞
"""
import json
import logging
import os
import sys
import unicodedata
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Any, Callable, Dict, Iterable, Optional, Sequence, Tuple

import hashlib
import pandas as pd
import polars as pl
import requests

# Íµ≠ÎÇ¥Ìú¥Ïû•ÏùºÏ°∞Ìöå API (Íµ≠ÎÇ¥Ï£ºÏãù-040)
_API_PATH_HOLIDAY = "/uapi/domestic-stock/v1/quotations/chk-holiday"
_TR_ID_HOLIDAY = "CTCA0903R"
_KST = timezone(timedelta(hours=9))
_CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.json")


def _is_open_day_via_api(target_dt: datetime) -> bool:
    """Íµ≠ÎÇ¥Ìú¥Ïû•ÏùºÏ°∞Ìöå API Ìò∏Ï∂úÎ°ú Í∞úÏû•Ïùº(Y) Ïó¨Î∂Ä ÌåêÎã®. kis_API_ohlcv_download_UtilsÎäî lazy importÎ°ú ÏàúÌôò Ï∞∏Ï°∞ Î∞©ÏßÄ."""
    from kis_API_ohlcv_download_Utils import DEFAULT_BASE_URL, KisClient, KisConfig

    cfg = load_config(_CONFIG_PATH)
    appkey = cfg.get("appkey")
    appsecret = cfg.get("appsecret")
    if not appkey or not appsecret:
        raise RuntimeError("configÏóê appkey/appsecretÏù¥ ÏóÜÏäµÎãàÎã§.")

    base_dir = os.path.dirname(_CONFIG_PATH)
    token_path = os.path.join(base_dir, "kis_token_main.json") if base_dir else "kis_token_main.json"

    client = KisClient(
        KisConfig(
            appkey=appkey,
            appsecret=appsecret,
            base_url=cfg.get("base_url") or DEFAULT_BASE_URL,
            custtype=cfg.get("custtype") or "P",
            market_div=cfg.get("market_div") or "J",
            token_cache_path=token_path,
        )
    )
    client.ensure_token()

    yyyymmdd = target_dt.strftime("%Y%m%d")
    url = f"{client.cfg.base_url}{_API_PATH_HOLIDAY}"
    headers = client._headers(tr_id=_TR_ID_HOLIDAY)
    params = {"BASS_DT": yyyymmdd, "CTX_AREA_NK": "", "CTX_AREA_FK": ""}

    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    j = r.json()
    if str(j.get("rt_cd")) != "0":
        raise RuntimeError(f"APIÏò§Î•ò: {j.get('msg1', '')}")

    output = j.get("output") or []
    if isinstance(output, dict):
        output = [output]
    row = output[0] if output else {}
    opnd_yn = str(row.get("opnd_yn", "N")).upper()
    return opnd_yn == "Y"


def is_holiday() -> bool:
    """
    Ïò§ÎäòÏù¥ Ìú¥Ïû•ÏùºÏù∏ÏßÄ Î∞òÌôòÌï©ÎãàÎã§.

    - configÏùò today_open_chk: {date: "YYYY-MM-DD", status: "open"|"holiday"}
    - dateÍ∞Ä Ïò§ÎäòÍ≥º Îã§Î•¥Î©¥ API Ìò∏Ï∂ú ÌõÑ Í≤∞Í≥ºÎ•º configÏóê Ï†ÄÏû•ÌïòÍ≥† Î∞òÌôò
    - dateÍ∞Ä Ïò§ÎäòÏù¥Î©¥ Ï†ÄÏû•Îêú statusÎ°ú ÌåêÎã®

    Returns:
        True: Ìú¥Ïû•Ïùº, False: Í∞úÏû•Ïùº
    """
    cfg = load_config(_CONFIG_PATH)
    today = datetime.now(_KST).strftime("%Y-%m-%d")

    chk = cfg.get("today_open_chk")
    if isinstance(chk, dict):
        stored_date = str(chk.get("date", ""))
        stored_status = str(chk.get("status", "")).lower()
    else:
        stored_date = ""
        stored_status = ""

    if stored_date == today and stored_status in ("open", "holiday"):
        return stored_status == "holiday"

    # API Ìò∏Ï∂ú ÌõÑ config Ï†ÄÏû•
    try:
        target_dt = datetime.now(_KST)
        is_open = _is_open_day_via_api(target_dt)
    except Exception:
        raise

    status = "open" if is_open else "holiday"
    cfg["today_open_chk"] = {"date": today, "status": status}
    save_config(cfg, _CONFIG_PATH)
    return not is_open


def load_config(config_path: str = "config.json", account_id: str | None = None) -> Dict[str, str]:
    """
    ÏÑ§Ï†ï ÌååÏùºÏùÑ ÏùΩÏñ¥ÏÑú ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôòÌï©ÎãàÎã§.
    
    Args:
        config_path: ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú (Í∏∞Î≥∏Í∞í: config.json)
        account_id: Í≥ÑÏ†ï ID (accounts ÌïòÏúÑÏóêÏÑú Ïò§Î≤ÑÎùºÏù¥Îìú)
        
    Returns:
        ÏÑ§Ï†ï Í∞íÎì§Ïù¥ Îã¥Í∏¥ ÎîïÏÖîÎÑàÎ¶¨
        
    Raises:
        FileNotFoundError: ÏÑ§Ï†ï ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùÑ Îïå
        json.JSONDecodeError: JSON ÌååÏã± Ïò§Î•ò
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"ÏÑ§Ï†ï ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {config_path}")
    
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    
    if account_id:
        accounts = config.get("accounts", {})
        if isinstance(accounts, dict) and account_id in accounts:
            base_cfg = {k: v for k, v in config.items() if k != "accounts"}
            merged = dict(base_cfg)
            merged.update(accounts.get(account_id, {}))
            if "cano" in merged:
                merged.setdefault("cano1", merged.get("cano", ""))
                merged.setdefault("cano2", merged.get("cano", ""))
            return merged
    return config


def resolve_account_config(config_path: str = "config.json", account_id: str | None = None) -> Dict[str, str]:
    """
    Í≥µÌÜµ ÏÑ§Ï†ï + Í≥ÑÏ†ïÎ≥Ñ Ïò§Î≤ÑÎùºÏù¥ÎìúÎ•º Î≥ëÌï©Ìïú ÏÑ§Ï†ïÏùÑ Î∞òÌôòÌï©ÎãàÎã§.
    """
    return load_config(config_path=config_path, account_id=account_id)


def stQueryDB(code: str, parquet_path: str | None = None) -> Dict[str, Any]:
    """
    1d ÌååÏºÄÏù¥Ìä∏ DBÏóêÏÑú Ï¢ÖÎ™©ÏΩîÎìú Í∏∞Ï§Ä ÏµúÏã† ÌñâÏùÑ Î∞òÌôòÌï©ÎãàÎã§.
    Î∞òÌôò ÌÇ§ Ïòà: open, high, low, close, volume, value, pdy_close, R_avr Îì±
    """
    if not code:
        return {}
    base_dir = os.path.dirname(os.path.abspath(__file__))
    path = parquet_path or os.path.join(base_dir, "data", "1d_data", "kis_1d_unified_parquet_DB.parquet")
    if not os.path.exists(path):
        return {}
    cols = [
        "date",
        "symbol",
        "code",
        "name",
        "open",
        "high",
        "low",
        "close",
        "volume",
        "value",
        "pdy_close",
        "R_avr",
    ]
    try:
        df = pl.read_parquet(path, columns=cols)
    except Exception:
        try:
            df = pl.read_parquet(path)
        except Exception:
            return {}
    if df.is_empty():
        return {}
    if "symbol" in df.columns:
        df = df.filter(pl.col("symbol").cast(pl.Utf8) == str(code))
    elif "code" in df.columns:
        df = df.filter(pl.col("code").cast(pl.Utf8) == str(code))
    if df.is_empty():
        return {}
    if "date" in df.columns:
        try:
            df = df.sort("date")
        except Exception:
            pass
    row = df.tail(1).to_dicts()[0]
    return row


def now_kst_str() -> str:
    kst = timezone(timedelta(hours=9))
    return datetime.now(kst).strftime("%y%m%d_%H%M%S")


class KSTFormatter(logging.Formatter):
    def formatTime(self, record, datefmt=None):
        kst = timezone(timedelta(hours=9))
        dt = datetime.fromtimestamp(record.created, tz=kst)
        if datefmt:
            return dt.strftime(datefmt)
        return dt.isoformat()


class _DropNoConsoleFilter(logging.Filter):
    """no_console=True Î†àÏΩîÎìúÎäî ÏΩòÏÜî Ìï∏Îì§Îü¨ÏóêÏÑú Ï†úÏô∏ (Ï§ëÎ≥µ Î∞©ÏßÄ)"""

    def filter(self, record: logging.LogRecord) -> bool:
        return not getattr(record, "no_console", False)


class LogManager:
    """
    Ï∂úÎ†• 3Îã®Í≥Ñ ÌÜµÌï© Î°úÍ±∞ (ÌôîÎ©¥ Ï∂úÎ†• Ìï≠ÏÉÅ Î≥¥Ïû•, ÌÖîÎ†àÍ∑∏Îû® Ï†ÑÏÜ° Ïãú Î∞òÎìúÏãú ÌôîÎ©¥ ÏÑ†Ìñâ)

    1Îã®Í≥Ñ log_screen : ÌôîÎ©¥Îßå
    2Îã®Í≥Ñ log       : ÌôîÎ©¥ + Î°úÍ∑∏ÌååÏùº
    3Îã®Í≥Ñ log_tm    : ÌôîÎ©¥ + Î°úÍ∑∏ + ÌÖîÎ†àÍ∑∏Îû®

    ÏÇ¨Ïö©Î≤ï:
        lm = LogManager(log_dir)
        lm.log_screen("ÌôîÎ©¥Îßå")
        lm.log("ÌôîÎ©¥+Î°úÍ∑∏")
        lm.log_tm("ÌôîÎ©¥+Î°úÍ∑∏+ÌÖîÎ†àÍ∑∏Îû®")

    prefix_callback: ÏãúÍ∞Ñ Ï†ëÎëêÏñ¥ Ìï®Ïàò (Ïòà: lambda: "[260215_012345]") - ÎØ∏ÏßÄÏ†ï Ïãú KST ÌòÑÏû¨ÏãúÍ∞Å ÏÇ¨Ïö©
    """

    def __init__(
        self,
        log_dir: str,
        log_name: str | None = None,
        run_tag: str | None = None,
        prefix_callback: Callable[[], str] | None = None,
    ):
        self.log_dir = log_dir
        os.makedirs(self.log_dir, exist_ok=True)

        if log_name:
            log_file = os.path.join(self.log_dir, log_name)
        else:
            timestamp = now_kst_str()
            log_file = os.path.join(self.log_dir, f"KIS_log_{timestamp}.log")
        self.log_file = log_file

        kst = timezone(timedelta(hours=9))

        def _default_prefix() -> str:
            return "[" + datetime.now(kst).strftime("%y%m%d_%H:%M:%S") + "]"

        self._prefix = prefix_callback if prefix_callback else _default_prefix

        self.logger = logging.getLogger("KISLogger")
        self.logger.setLevel(logging.INFO)
        self.logger.propagate = False

        if not self.logger.handlers:
            ch = logging.StreamHandler()
            ch.setLevel(logging.INFO)
            ch.addFilter(_DropNoConsoleFilter())

            fh = logging.FileHandler(log_file, encoding="utf-8")
            fh.setLevel(logging.INFO)

            formatter = KSTFormatter(
                "[%(asctime)s] %(message)s",
                datefmt="%y%m%d_%H:%M:%S",
            )
            ch.setFormatter(formatter)
            fh.setFormatter(formatter)

            self.logger.addHandler(ch)
            self.logger.addHandler(fh)

        if run_tag:
            try:
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write("\n" + "=" * 20 + "\n")
                    f.write(run_tag + "\n")
            except Exception:
                pass
        print(f"üìù Î°úÍ∑∏ ÌååÏùº: {log_file}")

    def _fmt(self, msg: str) -> str:
        """Ï†ëÎëêÏñ¥ + Î©îÏãúÏßÄ (log_screen, log_tm ÌôîÎ©¥ Ï∂úÎ†•Ïö©)"""
        return self._prefix() + " " + msg

    def log_screen(self, msg: str) -> None:
        """1Îã®Í≥Ñ: ÌôîÎ©¥Îßå Ï∂úÎ†•"""
        out = self._fmt(msg)
        sys.stdout.write(out + "\n")
        sys.stdout.flush()

    def log(self, msg: str) -> None:
        """2Îã®Í≥Ñ: ÌôîÎ©¥ + Î°úÍ∑∏ ÌååÏùº"""
        self.logger.info(msg)

    def log_tm(self, msg: str) -> None:
        """3Îã®Í≥Ñ: ÌôîÎ©¥ + Î°úÍ∑∏ + ÌÖîÎ†àÍ∑∏Îû® (ÌÖîÎ†àÍ∑∏Îû® Ï†ÑÏÜ° Ïãú Î∞òÎìúÏãú ÌôîÎ©¥ ÏÑ†Ìñâ)"""
        out = self._fmt(msg)
        sys.stdout.write(out + "\n")
        sys.stdout.flush()
        self.logger.info(msg, extra={"no_console": True})
        try:
            from telegMsg import tmsg

            tmsg(out, "-t")
        except ImportError:
            pass
        except Exception as e:
            self.logger.warning(f"ÌÖîÎ†àÍ∑∏Îû® Î©îÏãúÏßÄ Ï†ÑÏÜ° Ïã§Ìå®: {e}")

    def log_raw(self, msg: str) -> None:
        """2Îã®Í≥Ñ(raw): ÌôîÎ©¥ + Î°úÍ∑∏, msg Í∑∏ÎåÄÎ°ú (Ìò∏Ï∂úÏûêÍ∞Ä Ï†ëÎëêÏñ¥ Ìè¨Ìï®ÌïòÏó¨ Ï†ÑÎã¨, ÌååÏùºÏùÄ msg Í∑∏ÎåÄÎ°ú Í∏∞Î°ù)"""
        sys.stdout.write(msg + "\n")
        sys.stdout.flush()
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(msg + "\n")
        except Exception:
            pass

    def log_file_only(self, msg: str) -> None:
        """ÌååÏùºÎßå Í∏∞Î°ù (ÌôîÎ©¥ Ï∂úÎ†• ÏóÜÏùå)"""
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(msg + "\n")
        except Exception:
            pass

    def log_tm_raw(self, msg: str) -> None:
        """3Îã®Í≥Ñ(raw): ÌôîÎ©¥ + Î°úÍ∑∏ + ÌÖîÎ†àÍ∑∏Îû®, msg Í∑∏ÎåÄÎ°ú (Ìò∏Ï∂úÏûêÍ∞Ä Ï†ëÎëêÏñ¥ Ìè¨Ìï®ÌïòÏó¨ Ï†ÑÎã¨)"""
        sys.stdout.write(msg + "\n")
        sys.stdout.flush()
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(msg + "\n")
        except Exception:
            pass
        try:
            from telegMsg import tmsg

            tmsg(msg, "-t")
        except ImportError:
            pass
        except Exception as e:
            self.logger.warning(f"ÌÖîÎ†àÍ∑∏Îû® Î©îÏãúÏßÄ Ï†ÑÏÜ° Ïã§Ìå®: {e}")


class TeeStdout:
    def __init__(self, log_file: str):
        self.log_file = log_file
        self._file = open(log_file, "a", encoding="utf-8")
        self._stdout = sys.stdout

    def write(self, data: str) -> None:
        if data:
            self._file.write(data)
            self._file.flush()
        self._stdout.write(data)
        self._stdout.flush()

    def flush(self) -> None:
        self._file.flush()
        self._stdout.flush()

    def close(self) -> None:
        try:
            self._file.close()
        except Exception:
            pass


def display_width(text: str) -> int:
    width = 0
    for ch in text:
        if unicodedata.east_asian_width(ch) in ("W", "F"):
            width += 2
        else:
            width += 1
    return width


def pad_text(text: str, width: int, align: str = "left") -> str:
    text_width = display_width(text)
    if text_width >= width:
        return text
    pad = width - text_width
    if align == "right":
        return " " * pad + text
    if align == "center":
        left = pad // 2
        right = pad - left
        return " " * left + text + " " * right
    return text + " " * pad


def print_table(rows: list[dict], columns: list[str], align: dict[str, str],
                max_rows: int | None = None) -> None:
    """
    ÌïúÍ∏Ä Ìè≠ Î≥¥Ï†ï Ï†ïÎ†¨ ÌÖåÏù¥Î∏î Ï∂úÎ†•.
      max_rows=None : Ï†ÑÏ≤¥ Ìñâ Ï∂úÎ†•
      max_rows=20   : Ïïû 10Ìñâ + ... + Îí§ 10Ìñâ (Ï§ëÍ∞Ñ ÏÉùÎûµ)
    """
    if not rows:
        return
    str_rows = []
    widths = {col: display_width(col) for col in columns}
    for row in rows:
        srow = {}
        for col in columns:
            val = row.get(col, "")
            s = "" if val is None else str(val)
            srow[col] = s
            widths[col] = max(widths[col], display_width(s))
        str_rows.append(srow)
    # Ï∂úÎ†• ÎåÄÏÉÅ Ìñâ Í≤∞Ï†ï
    n = len(str_rows)
    if max_rows is not None and n > max_rows:
        half = max_rows // 2
        show_rows = str_rows[:half] + [None] + str_rows[-half:]
    else:
        show_rows = str_rows
    # Ìó§Îçî + Íµ¨Î∂ÑÏÑ†
    header = "  ".join(pad_text(col, widths[col], "left") for col in columns)
    print(header)
    print("  ".join("-" * widths[col] for col in columns))
    # Ìñâ Ï∂úÎ†•
    for srow in show_rows:
        if srow is None:
            print("  ".join(pad_text("...", widths[col], "center") for col in columns))
        else:
            print("  ".join(pad_text(srow[col], widths[col], align.get(col, "left")) for col in columns))
    print(f"[{n} rows x {len(columns)} columns]")


def save_config(config: Dict[str, str], config_path: str = "config.json") -> None:
    """
    ÏÑ§Ï†ïÏùÑ ÌååÏùºÏóê Ï†ÄÏû•Ìï©ÎãàÎã§.
    
    Args:
        config: Ï†ÄÏû•Ìï† ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
        config_path: ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú (Í∏∞Î≥∏Í∞í: config.json)
    """
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)


def get_config_value(config: Dict[str, str], key: str, default: Optional[str] = None) -> Optional[str]:
    """
    ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨ÏóêÏÑú Í∞íÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.
    
    Args:
        config: ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
        key: Í∞ÄÏ†∏Ïò¨ ÌÇ§
        default: Í∏∞Î≥∏Í∞í (ÌÇ§Í∞Ä ÏóÜÏùÑ Îïå)
        
    Returns:
        ÏÑ§Ï†ï Í∞í ÎòêÎäî Í∏∞Î≥∏Í∞í
    """
    return config.get(key, default)


def load_checkpoints(config_path: str = "config.json") -> Dict[str, str]:
    try:
        cfg = load_config(config_path)
    except FileNotFoundError:
        return {}
    return cfg.get("checkpoints", {}) or {}


def update_checkpoint(config_path: str, name: str, date_str: str) -> None:
    cfg = load_config(config_path)
    checkpoints = cfg.get("checkpoints", {}) or {}
    checkpoints[name] = date_str
    cfg["checkpoints"] = checkpoints
    save_config(cfg, config_path)


# =============================================================================
# KIS Data Layout (local + S3) + DuckDB helpers
# =============================================================================


def _default_local_root() -> str:
    return os.path.join(os.path.expanduser("~"), "Stoc_Kis", "data")


@dataclass(frozen=True)
class KisDataLayout:
    """
    ÌëúÏ§Ä Ï†ÄÏû• Íµ¨Ï°∞ Ï†ïÏùò
    - Î°úÏª¨: /home/ubuntu/Stoc_Kis/data (TEMP -> OUT -> S3)
    - S3: s3://tfttrain/KIS_DB/...
    """

    local_root: str = _default_local_root()
    s3_bucket: str = "tfttrain"
    s3_prefix: str = "KIS_DB"
    bucket_count_1m: int = 16

    # Local subpaths
    def local_temp_by_symbol_1d(self) -> str:
        return os.path.join(self.local_root, "temp", "by_symbol", "1d")

    def local_temp_by_symbol_1m(self) -> str:
        return os.path.join(self.local_root, "temp", "by_symbol", "1m")

    def local_out_1d_date(self, date_str: str) -> str:
        return os.path.join(
            self.local_root, "temp_out", "market_data", "1d", f"date={date_str}", "ohlcv.parquet"
        )

    def local_out_1m_date(self, date_str: str) -> str:
        return os.path.join(
            self.local_root, "temp_out", "market_data", "1m", f"date={date_str}", "ohlcv.parquet"
        )

    def local_out_1m_bucket(self, date_str: str, bucket: str) -> str:
        return os.path.join(
            self.local_root,
            "temp_out",
            "market_data",
            "1m",
            f"date={date_str}",
            f"bucket={bucket}",
            "ohlcv.parquet",
        )

    def local_features_1d_signal(self, date_str: str) -> str:
        return os.path.join(
            self.local_root, "temp_out", "features", "1d_signal", f"date={date_str}", "signal.parquet"
        )

    # S3 subpaths
    def s3_1d_date(self, date_str: str) -> str:
        return f"s3://{self.s3_bucket}/{self.s3_prefix}/market_data/1d/date={date_str}/ohlcv.parquet"

    def s3_1m_date(self, date_str: str) -> str:
        return f"s3://{self.s3_bucket}/{self.s3_prefix}/market_data/1m/date={date_str}/ohlcv.parquet"

    def s3_1m_bucket(self, date_str: str, bucket: str) -> str:
        return (
            f"s3://{self.s3_bucket}/{self.s3_prefix}/market_data/1m/"
            f"date={date_str}/bucket={bucket}/ohlcv.parquet"
        )

    def s3_features_1d_signal(self, date_str: str) -> str:
        return (
            f"s3://{self.s3_bucket}/{self.s3_prefix}/features/1d_signal/"
            f"date={date_str}/signal.parquet"
        )

    def s3_glob_1d(self) -> str:
        return f"s3://{self.s3_bucket}/{self.s3_prefix}/market_data/1d/date=*/ohlcv.parquet"

    def s3_glob_1m(self) -> str:
        return f"s3://{self.s3_bucket}/{self.s3_prefix}/market_data/1m/date=*/ohlcv.parquet"


def load_kis_data_layout(config_path: str = "config.json") -> KisDataLayout:
    """
    config.jsonÏóêÏÑú Ï†ÄÏû• Íµ¨Ï°∞ Í∏∞Î≥∏Í∞íÏùÑ ÏùΩÏñ¥ KisDataLayout Î∞òÌôò
    (ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©)
    """
    cfg: Dict[str, str] = {}
    try:
        cfg = load_config(config_path)
    except FileNotFoundError:
        cfg = {}

    return KisDataLayout(
        local_root=cfg.get("local_root", _default_local_root()),
        s3_bucket=cfg.get("s3_bucket", "tfttrain"),
        s3_prefix=cfg.get("s3_prefix", "KIS_DB"),
        bucket_count_1m=int(cfg.get("bucket_count_1m", 16)),
    )


def _tm(msg: str) -> None:
    """
    ÌÖîÎ†àÍ∑∏Îû® Î©îÏãúÏßÄ Ï†ÑÏÜ°
    - telegMsg.tmsg(msg, "-t") ÏÇ¨Ïö© (TFT_utils Î∞©Ïãù)
    """
    try:
        from telegMsg import tmsg

        tmsg(msg, "-t")
        
    except Exception:
        return


def send_telegram(msg: str) -> None:
    _tm(msg)


def symbol_master_path(layout: KisDataLayout) -> str:
    return os.path.join(layout.local_root, "admin", "symbol_master", "KRX_code.parquet")


def load_symbol_master(config_path: str = "config.json"):
    import pandas as pd

    layout = load_kis_data_layout(config_path)
    path = symbol_master_path(layout)
    if not os.path.exists(path):
        raise FileNotFoundError(f"symbol master parquet ÏóÜÏùå: {path}")
    return pd.read_parquet(path)


_SYMBOL_MASTER_CACHE = {}


def load_symbol_master_cached(config_path: str = "config.json"):
    if config_path in _SYMBOL_MASTER_CACHE:
        return _SYMBOL_MASTER_CACHE[config_path]
    df = load_symbol_master(config_path)
    _SYMBOL_MASTER_CACHE[config_path] = df
    return df


def _normalize_symbol_code(text: str) -> str:
    return str(text).strip().zfill(6)


def KRX_code(
    query: str, config_path: str = "config.json"
) -> tuple[str | None, str | None, str | None, str | None, str | None]:
    """
    ÏûÖÎ†•Îêú Ï¢ÖÎ™©ÏΩîÎìú/Î™ÖÏóê ÎåÄÌï¥ code, name, nxt, market, iscd_stat_cls_code(Ï¢ÖÎ™©ÏÉÅÌÉú) Î∞òÌôò
      * Ï¢ÖÎ™©ÏΩîÎìú/Î™Ö : Ï¢ÖÎ™©ÏΩîÎìú(6ÏûêÎ¶¨ ÎØ∏Îßå Ìè¨Ìï®) ÎòêÎäî Ï¢ÖÎ™©Î™Ö(ÌïúÍ∏Ä Ìè¨Ìï®)
      * ÏòÅÎ¨∏ÏùÄ ÏÜåÎ¨∏ÏûêÎèÑ Í≤ÄÏÉâ ÌóàÏö©
      * iscd_stat_cls_code: 57/59=30Î∂Ñ Îã®ÏùºÍ∞Ä, 58=ÏÉÅÏû•ÌèêÏßÄ ÏòàÏ†ï(Í±∞Îûò Ï†úÏô∏) Îì±
    """
    q = str(query).strip()
    if not q:
        return None, None, None, None, None
    df = load_symbol_master_cached(config_path)
    if "code" not in df.columns or "name" not in df.columns:
        return None, None, None, None, None

    def _market_from_row(row) -> str | None:
        if "market" not in row.index:
            return None
        v = row["market"]
        if pd.isna(v) or v == "" or str(v) in ("nan", "None"):
            return None
        return str(v)

    def _status_from_row(row) -> str | None:
        if "iscd_stat_cls_code" not in row.index:
            return None
        v = row["iscd_stat_cls_code"]
        if pd.isna(v) or v == "" or str(v) in ("nan", "None"):
            return None
        return str(v)

    # code match
    code = _normalize_symbol_code(q)
    exact = df[df["code"].astype(str).map(_normalize_symbol_code) == code]
    if not exact.empty:
        row = exact.iloc[0]
        nxt = str(row["nxt"]) if "nxt" in row.index else "N"
        return code, str(row["name"]), nxt, _market_from_row(row), _status_from_row(row)
    # name match (ÏòÅÎ¨∏ÏùÄ ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ ÏóÜÏù¥ Í≤ÄÏÉâ: q.upper()ÏôÄ ÎπÑÍµê)
    match = df[df["name"].astype(str).str.upper() == q.upper()]
    if not match.empty:
        row = match.iloc[0]
        code_val = _normalize_symbol_code(row["code"])
        nxt = str(row["nxt"]) if "nxt" in row.index else "N"
        return code_val, str(row["name"]), nxt, _market_from_row(row), _status_from_row(row)
    return None, None, None, None, None


def KRX_code_batch(
    symbols: list[str], config_path: str = "config.json"
) -> dict[str, str | None]:
    """
    Ïó¨Îü¨ Ï¢ÖÎ™©ÏΩîÎìúÏóê ÎåÄÌï¥ iscd_stat_cls_code(Ï¢ÖÎ™©ÏÉÅÌÉú) Î∞òÌôò. code -> status Îß§Ìïë.
    KRX_codeÏôÄ ÎèôÏùº Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ÏÇ¨Ïö©. mergeÎ°ú ÏùºÍ¥Ñ Ï°∞ÌöåÌïòÏó¨ ÏÑ±Îä• ÏµúÏ†ÅÌôî.
    """
    if not symbols:
        return {}
    df = load_symbol_master_cached(config_path)
    if "code" not in df.columns or "iscd_stat_cls_code" not in df.columns:
        return {s: None for s in symbols}
    lookup = pd.DataFrame(
        {"_q": symbols, "_norm": [_normalize_symbol_code(s) for s in symbols]}
    )
    master = df[["code", "iscd_stat_cls_code"]].copy()
    master["_norm"] = master["code"].astype(str).map(_normalize_symbol_code)
    master = master.drop_duplicates(subset=["_norm"], keep="first")
    merged = lookup.merge(master[["_norm", "iscd_stat_cls_code"]], on="_norm", how="left")
    result: dict[str, str | None] = {}
    for orig, val in zip(merged["_q"], merged["iscd_stat_cls_code"]):
        if pd.isna(val) or str(val) in ("", "nan", "None"):
            result[orig] = None
        else:
            result[orig] = str(val)
    return result


def resolve_symbol(query: str, df):
    q = query.strip()
    if not q:
        return None, []
    # code exact match
    exact = df[df["code"] == q]
    if not exact.empty:
        row = exact.iloc[0]
        return row, []
    # name contains
    matches = df[df["name"].str.contains(q, na=False)]
    if len(matches) == 1:
        return matches.iloc[0], []
    return None, matches


def bucket_for_symbol(symbol: str, bucket_count: int = 16) -> str:
    """
    symbolÏùÑ ÏïàÏ†ïÏ†ÅÏúºÎ°ú Î≤ÑÌÇ∑ Î∂ÑÎ∞∞ (00 ~ N-1)
    - hash(symbol) % N (SHA1 ÏÇ¨Ïö©)
    """
    h = hashlib.sha1(symbol.encode("utf-8")).hexdigest()
    idx = int(h, 16) % bucket_count
    return f"{idx:02d}"


def s3_paths_1m_for_symbols(
    layout: KisDataLayout,
    date_list: Sequence[str],
    symbols: Sequence[str],
) -> Sequence[str]:
    """
    1m Ï°∞ÌöåÏö© S3 Í≤ΩÎ°ú Î¶¨Ïä§Ìä∏ (date + bucket Ï†úÌïú)
    """
    buckets = {bucket_for_symbol(sym, layout.bucket_count_1m) for sym in symbols}
    paths = []
    for d in date_list:
        for b in sorted(buckets):
            paths.append(layout.s3_1m_bucket(d, b))
    return paths


def list_s3_1d_paths(layout: KisDataLayout, date_from: str, date_to: str) -> list[str]:
    import boto3

    s3 = boto3.client("s3")
    prefix = f"{layout.s3_prefix}/market_data/1d/date="
    paginator = s3.get_paginator("list_objects_v2")
    paths: list[str] = []
    for page in paginator.paginate(Bucket=layout.s3_bucket, Prefix=prefix):
        for obj in page.get("Contents", []) or []:
            key = obj.get("Key", "")
            if not key.endswith("/ohlcv.parquet"):
                continue
            try:
                part = key.split("date=", 1)[1]
                date_str = part.split("/", 1)[0]
            except Exception:
                continue
            if date_from <= date_str <= date_to:
                paths.append(f"s3://{layout.s3_bucket}/{key}")
    return sorted(paths)


def _kr_tick_size(price: float) -> int:
    if price < 1000:
        return 1
    if price < 5000:
        return 5
    if price < 10000:
        return 10
    if price < 50000:
        return 50
    if price < 100000:
        return 100
    if price < 500000:
        return 500
    return 1000


def calc_limit_up_price(prev_close: float) -> float:
    if prev_close is None:
        return float("nan")
    try:
        if prev_close != prev_close:
            return float("nan")
    except Exception:
        return float("nan")
    raw = prev_close * 1.3
    tick = _kr_tick_size(raw)
    return round(raw / tick) * tick


def _configure_duckdb_s3(con, config_path: str = "config.json") -> None:
    def _imds_token() -> Optional[str]:
        try:
            import requests

            r = requests.put(
                "http://169.254.169.254/latest/api/token",
                headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
                timeout=1,
            )
            if r.status_code == 200:
                return r.text
        except Exception:
            return None
        return None

    def _imds_region() -> Optional[str]:
        try:
            import requests

            token = _imds_token()
            headers = {"X-aws-ec2-metadata-token": token} if token else {}
            r = requests.get(
                "http://169.254.169.254/latest/dynamic/instance-identity/document",
                headers=headers,
                timeout=1,
            )
            if r.status_code == 200:
                data = r.json()
                return data.get("region")
        except Exception:
            return None
        return None

    try:
        con.execute("INSTALL httpfs;")
    except Exception:
        pass
    con.execute("LOAD httpfs;")
    cfg: Dict[str, str] = {}
    try:
        cfg = load_config(config_path)
    except FileNotFoundError:
        cfg = {}
    region = (
        cfg.get("s3_region")
        or os.getenv("AWS_REGION")
        or os.getenv("AWS_DEFAULT_REGION")
        or _imds_region()
    )
    access_key = cfg.get("aws_access_key_id") or os.getenv("AWS_ACCESS_KEY_ID")
    secret_key = cfg.get("aws_secret_access_key") or os.getenv("AWS_SECRET_ACCESS_KEY")
    session_token = cfg.get("aws_session_token") or os.getenv("AWS_SESSION_TOKEN")
    endpoint = cfg.get("s3_endpoint") or os.getenv("AWS_S3_ENDPOINT")
    try:
        con.execute("SET s3_use_instance_profile=true;")
    except Exception:
        pass
    if region:
        con.execute(f"SET s3_region='{region}';")
    if not access_key or not secret_key:
        try:
            import boto3

            creds = boto3.Session().get_credentials()
            if creds:
                frozen = creds.get_frozen_credentials()
                access_key = access_key or frozen.access_key
                secret_key = secret_key or frozen.secret_key
                session_token = session_token or frozen.token
        except Exception:
            pass

    if access_key and secret_key:
        con.execute(f"SET s3_access_key_id='{access_key}';")
        con.execute(f"SET s3_secret_access_key='{secret_key}';")
    if session_token:
        con.execute(f"SET s3_session_token='{session_token}';")
    if endpoint:
        con.execute(f"SET s3_endpoint='{endpoint}';")


def duckdb_read_1d_s3(
    layout: KisDataLayout,
    date_from: str,
    date_to: str,
    symbols: Sequence[str],
) -> "object":
    """
    DuckDBÎ°ú S3 1d Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå (date ÌååÌã∞ÏÖò)
    """
    import duckdb
    import pandas as pd

    paths = list_s3_1d_paths(layout, date_from, date_to)
    if not paths:
        cols = ["date", "symbol", "open", "high", "low", "close", "volume", "value"]
        return duckdb.from_df(pd.DataFrame(columns=cols)).df()

    symbols_sql = ",".join([f"'{s}'" for s in symbols])
    files_sql = "[" + ",".join([f"'{p}'" for p in paths]) + "]"
    query = f"""
    SELECT date, symbol, open, high, low, close, volume, value
    FROM read_parquet({files_sql})
    WHERE date BETWEEN '{date_from}' AND '{date_to}'
      AND symbol IN ({symbols_sql})
    ORDER BY symbol, date
    """
    con = duckdb.connect()
    try:
        _configure_duckdb_s3(con)
        return con.execute(query).df()
    finally:
        con.close()


def duckdb_read_1m_s3(
    layout: KisDataLayout,
    date_list: Sequence[str],
    symbols: Sequence[str],
) -> "object":
    """
    DuckDBÎ°ú S3 1m Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå (date + bucket Ï†úÌïú)
    """
    import duckdb

    symbols_sql = ",".join([f"'{s}'" for s in symbols])
    date_sql = ",".join([f"'{d}'" for d in date_list])
    query = f"""
    SELECT date, ts, symbol, open, high, low, close, volume, value
    FROM read_parquet('{layout.s3_glob_1m()}', hive_partitioning=1)
    WHERE date IN ({date_sql})
      AND symbol IN ({symbols_sql})
    ORDER BY symbol, ts
    """
    con = duckdb.connect()
    try:
        _configure_duckdb_s3(con)
        return con.execute(query).df()
    finally:
        con.close()
